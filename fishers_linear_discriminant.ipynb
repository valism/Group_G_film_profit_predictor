{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d056b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydataset import data\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fomlads.data.external import import_for_classification\n",
    "\n",
    "from fomlads.model.classification import project_data\n",
    "from fomlads.model.classification import maximum_separation_projection\n",
    "\n",
    "from fomlads.plot.exploratory import plot_scatter_array_classes\n",
    "from fomlads.plot.exploratory import plot_class_histograms\n",
    "from fomlads.model.classification import fisher_linear_discriminant_projection\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ab6b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 7, 8, 9], [0, 1, 2, 3, 4]]\n",
      "____________________________________\n",
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# I wrote the get_fisher_histogram function to avoid repeated code\n",
    "# fisher_linear_discriminant_projection, project_data, and plot_class_histograms are all from fomlads\n",
    "def get_fisher_histogram(inputs, targets):\n",
    "    \"\"\"\n",
    "    Returns the projected histogram using Fishers model\n",
    "    \"\"\"\n",
    "    \n",
    "    w = fisher_linear_discriminant_projection(inputs, targets)\n",
    "    projected_inputs = project_data(inputs, w)\n",
    "    ax = plot_class_histograms(projected_inputs, targets)\n",
    "\n",
    "   \n",
    "    \n",
    "def get_fishers_predictions(inputs, weights): \n",
    "    \n",
    "    \"\"\"\n",
    "    Predicts the targets based on inputs and weights.\n",
    "    The weights are caluculated using Fishers criterion.\n",
    "    \"\"\"\n",
    "        \n",
    "    m = np.mean(inputs, axis = 0)\n",
    "    predictions = []\n",
    "    \n",
    "    for x in inputs:\n",
    "        y = np.matmul(weights.T, x-m) # y = w.T * (x - m)\n",
    "        if y > 0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "\n",
    "    return predictions\n",
    "        \n",
    "def get_accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculates what proportion of the guesses are correct.\n",
    "    For example, if 80% of the values match, returns 0.8.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    for i in range(len(targets)):\n",
    "        if predictions[i] == targets[i]:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "    accuracy = sum(scores)/len(targets)\n",
    "    return accuracy\n",
    "\n",
    "def get_train_test_split(df, train_size = 0.7):\n",
    "    \n",
    "    \"\"\"\n",
    "    Splits the dataframe into a training set and a testing set.\n",
    "    train_size determiens how much of the data goes into the trainig set (0.7 means 70%).\n",
    "    Returns the training set and testing set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Code for sampling adapted from: https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "    \n",
    "    train = df.sample(frac = train_size) # Use random_state to fix a seed value\n",
    "    test = df.drop(train.index)\n",
    "    \n",
    "#     x_train = train.drop(y_label, axis = 1)\n",
    "#     y_train = train[y_label]\n",
    "    \n",
    "#     x_test = test.drop(y_label, axis = 1)\n",
    "#     y_test = test[y_label]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Code adapted from: https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length\n",
    "def split(a, n):\n",
    "    \"\"\"\n",
    "    Splits a list into n parts of approximately equal length.\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        raise Exception(\" Cannot split a list into 0 parts, enter a valid input for n\")\n",
    "    elif n == 1:\n",
    "        return a\n",
    "    else:    \n",
    "        k, m = divmod(len(a), n)\n",
    "        return list(a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "\n",
    "def get_fold_indices(df, num_folds = 10):\n",
    "    \"\"\"\n",
    "    Retruns the indices for each iteration of cross validation.\n",
    "    For example, if input is has 10 rows and needs 5 folds:\n",
    "    the first iteration has indices [0, 1] for testing and [2, 3, 4, 5, 6, 7, 8, 9] for training\n",
    "    the second iteration has indices [2, 3] for testing and [0, 1, 4, 5, 6, 7, 8, 9] for training \n",
    "    and so on.\n",
    "    \n",
    "    train_indices[0] will give you the row numbers of the training data for the first iteration  \n",
    "    val_indices[0] will give you the row numbers of the validation data for the first iteration  \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    indices = df.index.values.tolist()\n",
    "    N = len(indices)   \n",
    "        \n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    if num_folds == 0 or num_folds == 1:\n",
    "        raise Exception(\" Cannot cross validate with less than 2 folds\")\n",
    "    else:      \n",
    "    \n",
    "        splits = split(indices, num_folds)\n",
    "\n",
    "        for i in range(num_folds):\n",
    "            v = splits[i]\n",
    "\n",
    "            t = splits[:i] + splits[i+1:]        \n",
    "            t =  [item for sublist in t for item in sublist]\n",
    "\n",
    "\n",
    "\n",
    "            train_indices.append(t)\n",
    "            val_indices.append(v)\n",
    "\n",
    "\n",
    "        return train_indices, val_indices\n",
    "        \n",
    "        \n",
    "# This is just to check that get_fold_indices() works.    \n",
    "x = np.arange(10)\n",
    "dfx = pd.DataFrame(x, columns = [\"x\"])\n",
    "t,v = get_fold_indices(dfx, 2)\n",
    "\n",
    "print(t)\n",
    "print(\"____________________________________\")\n",
    "print(v)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910d525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ######################################################################\n",
    "# # Iris data from tutorial, just to check if fisher model is working.\n",
    "\n",
    "# ifname = 'iris.data'\n",
    "\n",
    "# # reimport data, just 2 classes\n",
    "# classes = [\"Iris-setosa\", \"Iris-versicolor\"]\n",
    "# inputs, targets, field_names, classes = import_for_classification(ifname, classes=classes)\n",
    "# N = len(inputs)\n",
    "\n",
    "# randomize = np.arange(len(targets))\n",
    "# np.random.shuffle(randomize)\n",
    "# inputs = inputs[randomize]\n",
    "# targets = targets[randomize]\n",
    "\n",
    "\n",
    "# x_train = inputs[0:int(N*0.7)]\n",
    "# y_train = targets[0:int(N*0.7)]\n",
    "\n",
    "# x_test = inputs[int(N*0.7):]\n",
    "# y_test = targets[int(N*0.7):]\n",
    "\n",
    "\n",
    "# # Using fomlads model\n",
    "# fishers_weights = fisher_linear_discriminant_projection(inputs,targets)\n",
    "# fishers_predictions = get_fishers_predictions(x_test, fishers_weights)\n",
    "# accuracy = get_accuracy(fishers_predictions, y_test)\n",
    "# print(f\"Fomlads score = {accuracy * 100}%\")\n",
    "\n",
    "\n",
    "# # Using sklearn model\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# lda.fit(x_train, y_train)\n",
    "# lda_score = lda.score(x_test, y_test)\n",
    "\n",
    "\n",
    "# print(f\"sklearn score = {lda_score * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "106c7dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score = 71.77%\n",
      "sklearn score = 71.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####################################################################\n",
    "# Our dataset\n",
    "\n",
    "df = pd.read_csv(\"profit_x_y.csv\")\n",
    "df = df.drop(\"Unnamed: 0\", axis =1 )\n",
    "\n",
    "\n",
    "df = df.drop(\"title_x\", axis = 1)\n",
    "df = df.drop(\"title_y\", axis = 1)\n",
    "df = df.drop(\"profit_x\", axis = 1)\n",
    "df = df.drop(\"profit_y\", axis = 1)\n",
    "\n",
    "# Had to remove gross income since it can be used with budget to directly get profit,\n",
    "# and an unreleased movie will not have a known income.\n",
    "\n",
    "df = df.drop(\"worlwide_gross_income_x\", axis = 1)\n",
    "df = df.drop(\"worlwide_gross_income_y\", axis = 1)\n",
    "\n",
    "# Using a filter such as this one (or action movies only etc.) gives an error \n",
    "# about a singular matrix when calculating the weights. I'm not sure what's causing it.\n",
    "\n",
    "# df = df[df[\"LANGUAGE_English_x\"] == 1]\n",
    "\n",
    "\n",
    "# df_train will be used for cross validation, and df_test will be used for final testing\n",
    "df_train, df_test = get_train_test_split(df,train_size = 0.8)\n",
    "\n",
    "# Re-calculates the indices because pandas normally keeps the original indices when to take a section of a dataframe\n",
    "df_train.reset_index(inplace=True, drop=True) \n",
    "df_test.reset_index(inplace=True, drop=True) \n",
    "\n",
    "\n",
    "\n",
    "# Gets the row numbers for the training folds and the validaition folds for each iteration.\n",
    "# tf_indices -> indices for training folds for each iteration\n",
    "# vf_indices -> indices for validation folds for each iteration\n",
    "tf_indices, vf_indices = get_fold_indices(df_train, num_folds = 10)\n",
    "\n",
    "# Keeps track of the scores for each iteration of cross validation.\n",
    "cross_validation_scores = []\n",
    "\n",
    "# Performs cross validation\n",
    "for i in range(len(tf_indices)):  \n",
    "\n",
    "    # Training and validation data for the current iteration/\n",
    "    train_fold_df = df_train.iloc[tf_indices[i]]\n",
    "    validation_fold_df = df_train.iloc[vf_indices[i]]\n",
    "    \n",
    "    # X and y data for training fold\n",
    "    train_fold_x = train_fold_df.drop(\"profit_xy\", axis = 1).to_numpy()\n",
    "    train_fold_y = train_fold_df[\"profit_xy\"].to_numpy()\n",
    "    \n",
    "    # X and y data for validation fold\n",
    "    validation_fold_x = validation_fold_df.drop(\"profit_xy\", axis = 1).to_numpy()\n",
    "    validation_fold_y = validation_fold_df[\"profit_xy\"].to_numpy()\n",
    "    \n",
    "    # Gets weights from training fold\n",
    "    fishers_weights = fisher_linear_discriminant_projection(train_fold_x, train_fold_y)\n",
    "    \n",
    "    # Makes predictions using weights calculated from training fold\n",
    "    fishers_predictions = get_fishers_predictions(validation_fold_x, fishers_weights)\n",
    "\n",
    "    # Calucaltes accuracy for current iteration and appends to list\n",
    "    fold_accuracy = get_accuracy(fishers_predictions, validation_fold_y)\n",
    "    cross_validation_scores.append(fold_accuracy)\n",
    "    \n",
    "\n",
    "average_cv_percent = round(100 * sum(cross_validation_scores)/len(cross_validation_scores),2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Using sklearn model for comparison\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(df_train.drop(\"profit_xy\", axis = 1), df_train[\"profit_xy\"])\n",
    "lda_score = lda.score(df_test.drop(\"profit_xy\", axis = 1), df_test[\"profit_xy\"])\n",
    "lda_score = round(lda_score, 2)\n",
    "\n",
    "print(f\"Average cross validation score = {average_cv_percent}%\")\n",
    "print(f\"sklearn score = {lda_score * 100}%\")\n",
    "df_t, df_v = get_fold_indices(df,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using sklearn, compares fishers model (LinearDiscriminantAnalysis), logistic regression and support vector machine (LinearSVC).\n",
    "\n",
    "# # Creats and fits models to traning data\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# lda.fit(x_train, y_train)\n",
    "\n",
    "# log_reg = LogisticRegression()\n",
    "# log_reg.fit(x_train, y_train)\n",
    "\n",
    "# svm = LinearSVC()\n",
    "# svm.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# # Makes predictions on using data\n",
    "# y_pred_lda = lda.decision_function(x_test)\n",
    "# y_pred_logistic = log_reg.decision_function(x_test)\n",
    "# y_pred_svm = svm.decision_function(x_test)\n",
    "\n",
    "\n",
    "# # Gets the ROC curves for each model\n",
    "# # Using the \"score\" method is not a good metric since guessing \"1\" everytime will still give a score of ~50%. \n",
    "\n",
    "# # fpr -> Fale Positive Rate\n",
    "# # tpr -> True Positive Rate\n",
    "# # Can ignore threhsold, I only included it beacuse the YouTube tutorial had it.\n",
    "# fpr_lda, tpr_lda, threshold_lda = roc_curve(y_test, y_pred_lda)\n",
    "# fpr_logistic, tpr_logistic, threshold_logistic = roc_curve(y_test, y_pred_logistic)\n",
    "# fpr_svm, tpr_svm, threshold_svm = roc_curve(y_test, y_pred_svm)\n",
    "\n",
    "# # Caclulates the area under the ROC curve for each of the models/\n",
    "# auc_lda = auc(fpr_lda, tpr_lda)\n",
    "# auc_logistic = auc(fpr_logistic, tpr_logistic)\n",
    "# auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "\n",
    "# # Plots the ROC curves.\n",
    "# plt.figure()\n",
    "# plt.plot(fpr_lda, tpr_lda, label = f\"Fisher ( Area = {round(auc_lda,3)} )\")\n",
    "# plt.plot(fpr_logistic, tpr_logistic, label = f\"Logistic Regression( Area = {round(auc_logistic,3)} )\")\n",
    "# plt.plot(fpr_svm, tpr_svm, label = f\"Support Vector Machine( Area = {round(auc_svm,3)} )\")\n",
    "\n",
    "# plt.xlabel(\"False positive rate\")\n",
    "# plt.ylabel(\"True positive rate\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7b388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a22c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3334100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
